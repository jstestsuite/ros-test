#!/usr/bin/env python
#################################################################################
# Copyright 2018 ROBOTIS CO., LTD.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#################################################################################

# Authors: Gilbert #

import rospy
import os
import json
import numpy as np
import random
import time
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray
from src.turtlebot3_ddpg.environment_stage_1 import Env
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.layers import Dense, Dropout, Activation, Input

from tensorflow import keras

from tensorflow.keras.models import Model


EPISODES = 3000
GAMMA = 0.99
TAU = 0.001

class ReinforceAgent():
    def __init__(self, state_size, action_size):
        self.pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        self.dirPath = self.dirPath.replace('turtlebot3_ddpg/nodes', 'turtlebot3_ddpg/save_model/stage_1_')
        self.result = Float32MultiArray()
        self.load_model = False
        self.load_episode = 0
        self.state_size = state_size
        self.action_size = action_size
        self.episode_step = 6000
        self.target_update = 2000
        self.learning_rate = 0.00025
        self.epsilon = 1.0
        self.epsilon_decay = 0.99
        self.epsilon_min = 0.05
        self.batch_size = 32
        self.train_start = 32
        self.memory = deque(maxlen=1000000)


	self.TAU = 0.001     #Target Network HyperParameters
    	self.LRA = 0.0001    #Learning rate for Actor
	self.LRC = 0.001 #Lerning rate for Critic

        self.model, self.weights, self.state2 = self.buildActor()
        self.target_model, self.target_weights, self.target_state = self.buildActor()

        self.critic, self.state, self.action = self.buildCritic()
        self.target_critic, _, _ = self.buildCritic()

        self.updateTargetModel()

#added

        self.sess = tf.Session()
        
        self.action_gradient = tf.placeholder(tf.float32,[None, 2])
        self.params_grad = tf.gradients(self.model.output, self.model.weights, -self.action_gradient)
        grads = zip(self.params_grad, self.model.weights)
        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)
        self.sess.run(tf.global_variables_initializer())

        self.action_grads = tf.gradients(self.critic.output, self.action) #GRADIENTS for policy update
#end add



    def buildCritic(self):
        s = keras.layers.Input(shape=(14,), name='state')

        d1 = Dense(512, activation='relu')(s)


        a = keras.layers.Input(shape=(2,), name='action')
        x = keras.layers.concatenate([d1, a])

        x = Dense(512, activation='relu')(x)
        x = Dense(512, activation='relu')(x)



        q_val= Dense(1, activation='sigmoid', name='main_output')(x)

        critic = Model(inputs=[s, a], outputs=[q_val])
        #critic.summary()

        critic.compile(optimizer=tf.train.AdamOptimizer(), 
              loss='mse',
              metrics=['accuracy'])
        return critic, s, a

    def buildActor(self):
        state = keras.layers.Input(shape=(14,), name='main_input')


        x = Dense(512, activation='relu')(state)
        x = Dense(512, activation='relu')(x)
        x = Dense(512, activation='relu')(x)


        lin_vel = Dense(1, activation='sigmoid', name='lin_vel')(x)
        ang_vel = Dense(1, activation='tanh', name='ang_vel')(x)
        output = keras.layers.concatenate([lin_vel, ang_vel])

        actor = Model(inputs=[state], outputs=[output])
	#actor.summary()
        
        actor.compile(optimizer=tf.train.AdamOptimizer(), 
              loss='mse',
              metrics=['accuracy'])

        return actor, actor.trainable_weights, state


#added
    def train(self, states, action_grads):
        self.sess.run(self.optimize, feed_dict={
            self.state2: states,
            self.action_gradient: action_grads
        })

    def target_train(self):
        actor_weights = self.model.get_weights()
        actor_target_weights = self.target_model.get_weights()
        for i in xrange(len(actor_weights)):
            actor_target_weights[i] = self.TAU * actor_weights[i] + (1 - self.TAU)* actor_target_weights[i]
        self.target_model.set_weights(actor_target_weights)


    def target_train_c(self):
        critic_weights = self.target_critic.get_weights()
        critic_target_weights = self.target_critic.get_weights()
        for i in xrange(len(critic_weights)):
            critic_target_weights[i] = self.TAU * critic_weights[i] + (1 - self.TAU)* critic_target_weights[i]
	self.target_critic.set_weights(critic_target_weights)

    def gradients(self, states, actions):
        grad = self.sess.run(self.action_grads, feed_dict={
            self.state: states,
            self.action: actions
	})[0]
	return grad

#end add

#Need to update

#should be OK
    def updateTargetModel(self):
        #self.target_model.set_weights(self.model.get_weights())
        #self.target_critic.set_weights(self.critic.get_weights())
	self.target_train()
	self.target_train_c()


#should be OK
    def getAction(self, state):
        if np.random.rand() <= self.epsilon:
            self.q_value = [random.uniform(0, 1.0), random.uniform(-1.0, 1.0)]
            return self.q_value
        else:
            q_value = self.target_model.predict(state.reshape(1, len(state)))
	    q_value= q_value.flatten()
            self.q_value = q_value
            return q_value


#should be OK
    def appendMemory(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))


#need to analyze
####################### Put the new train and stuff in here
###################### also make sure you actually understand what you are doing...

    def trainModel(self, target=False):
        env.pause_proxy()
        mini_batch = random.sample(self.memory, self.batch_size)
        X_batch = np.empty(([0,14]), dtype=np.float64)
        X_batch2 = np.empty(([0,2]), dtype=np.float64)
        Y_batch = np.empty((0), dtype=np.float64)

        for i in range(self.batch_size):
            states = mini_batch[i][0]
            actions = mini_batch[i][1]
            rewards = mini_batch[i][2]
            next_states = mini_batch[i][3]
            dones = mini_batch[i][4]
	    #print len(states), "\n"
            q_value = self.model.predict(states.reshape(1, len(states)))


            next_target = self.target_model.predict(next_states.reshape(1, len(next_states)))

	    next_target = next_target[0][:]
            next_target = next_target.reshape(1, len())

            
	    next_q = self.target_critic.predict([next_states.reshape(1, len(next_states)), next_target ])

            Y_sample = rewards+GAMMA*next_q

	    test = states.reshape(1, len(next_states))
            X_batch = np.append(X_batch, test, axis = 0)
            X_batch2 = np.append(X_batch2, np.asarray(actions).reshape(1,2), axis = 0)   


            Y_batch = np.append(Y_batch, Y_sample)



        self.critic.fit([X_batch, X_batch2], Y_batch, batch_size=self.batch_size, epochs=1, verbose=0)
	with tf.Session() as sess:
	     #grads=self.action_grads, feed_dict={
            	#self.state: states,
            	#self.action: actions}	
		grads =self.gradients(X_batch, X_batch2)
	     #print X_batch.shape
	     #print grads.shape
		self.train(X_batch, grads)

        env.unpause_proxy()


#need to analyze
if __name__ == '__main__':
    rospy.init_node('turtlebot3_dqn_stage_1')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    get_action = Float32MultiArray()
    past_action = [0.0,0.0]
    state_size = 12
    action_size = 5

    env = Env(action_size)

    agent = ReinforceAgent(state_size, action_size)
    scores, episodes = [], []
    global_step = 0
    start_time = time.time()

    for e in range(agent.load_episode + 1, EPISODES):
        done = False
        state = env.reset()
        score = 0
	
        for t in range(agent.episode_step):
            action = agent.getAction(state)
	    #print action
	    #print "ACTION IS PRINTED"
            next_state, reward, done = env.step(action, past_action)
	    #print len(state), "\n"
            agent.appendMemory(state, action, reward, next_state, done)

            if len(agent.memory) >= agent.train_start:
                if global_step <= agent.target_update:
                    agent.trainModel()
                else:
                    agent.trainModel(True)

            score += reward
            state = next_state
            get_action.data = [action, score, reward]
            pub_get_action.publish(get_action)
	    past_action = action
            #if e % 10 == 0:#
           #     agent.model.save(agent.dirPath + str(e) + '.h5')#
             #   with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
            #        json.dump(param_dictionary, outfile)

            if t >= 400:
                rospy.loginfo("Time out!!")
                done = True

            if done:
                result.data = [score, np.max(agent.q_value)]
                pub_result.publish(result)
                agent.updateTargetModel()
                scores.append(score)
                episodes.append(e)
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)

                rospy.loginfo('Ep: %d score: %.2f memory: %d epsilon: %.2f time: %d:%02d:%02d',
                              e, score, len(agent.memory), agent.epsilon, h, m, s)
                param_keys = ['epsilon']
                param_values = [agent.epsilon]
                param_dictionary = dict(zip(param_keys, param_values))
                break

            global_step += 1
           # if global_step % agent.target_update == 0:
            #    rospy.loginfo("UPDATE TARGET NETWORK")

        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay
